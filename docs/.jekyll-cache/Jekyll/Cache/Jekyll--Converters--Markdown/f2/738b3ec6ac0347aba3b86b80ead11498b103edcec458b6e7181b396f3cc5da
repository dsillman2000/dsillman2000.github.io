I"<p>Â Â Â Â  Iâ€™ve spent the past two weeks just starting to get a handle
on how to use the popular low-level, memory-safe programming language Rust. In general,
it gives me C++ vibes when Iâ€™m programming in it. That said, I think that it could have
good potential as a machine learning development environment, so long as it is
supplied with the appropriate external libraries (or â€œcrates,â€ to use Rustâ€™s
terminology.)</p>

<p><br />
Â Â Â Â  So, to get a handle on how I could use Rust specifically
for machine learning purposes, I did some (very brief) research into what tools
might be useful. I knew that I wanted to make use of an <em>auto-differentiation</em>
tool, as this is essentially the bedrock of a machine learning dev environment.
Auto-differentiation allows us to compute partial derivatives of the output of
our models such that we can tweak and tune them into being better predictors. I looked
over three different auto-differentiation crates:</p>

<ul>
  <li><a href="https://link-url-here.org">easy_ml</a></li>
  <li><code class="language-plaintext highlighter-rouge">[autodiff](https://crates.io/crates/autodiff)</code> : The problem with using this crate
in particular is that it is mostly tailored toward calculus on scalar functions,
whereas most machine learning applications require higher-dimensional differentiation
in the form of vectors, matrices and scalars. I donâ€™t know if <code class="language-plaintext highlighter-rouge">autodiff</code> is necessarily
incapable of implementing these features, but most of their examples and documentation
emphasize their scalar equivalents.</li>
  <li><a href="https://link-url-here.org">autograd</a></li>
</ul>
:ET